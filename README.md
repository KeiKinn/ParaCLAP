# ParaCLAP – Towards a general language-audio model for computational paralinguistic tasks

![official](https://img.shields.io/badge/Official_Repo-green) ![paper](https://img.shields.io/badge/Paper-InterSpeech_2024-blue) ![arxiv](https://img.shields.io/badge/arxiv-2406.07203-red)

This repo includes the official PyTorch implementation of *ParaCLAP – Towards a general language-audio model for computational paralinguistic tasks*

## Abstract
Contrastive language-audio pretraining (CLAP) has recently emerged as a method for making audio analysis more generalisable. Specifically, CLAP-style models are able to ‘answer’ a diverse set of language queries, extending the capabilities of audio models beyond a closed set of labels. However, CLAP relies on a large set of (audio, query) pairs for pretraining. While such sets are available for general audio tasks, like captioning or sound event detection, there are no datasets with matched audio and text queries for computational paralinguistic (CP) tasks. As a result, the community relies on generic CLAP models trained for general audio with limited success. In the present study, we explore training considerations for ParaCLAP, a CLAP-style model suited to CP, including a novel process for creating audio-language queries. We demonstrate its effectiveness on a set of computational paralinguistic tasks, where it is shown to surpass the performance of open-source state-of-the-art models.

## Instruction

The goal of this work is to create a CLAP-style model for computational paralinguistics. This is done by training an acoustic model (`audeering-w2v2-emo`) and a text model (`bert-base-uncased`) with the CLIP objective (contrastive loss).

The text-audio pairs are generated through a novel art of templating
which accepts as input `eGeMAPS` features and generates pseudo-captions.

The templates are generated by `preprocessing/template_creation.py`. This script can be adapted to generate more pseudo-captions and control which variables are used.

The remaining scripts in this repository can be used as follows:
+ `train.py` trains the CLAP model
+ `features.py` extracts `eGeMAPS` features for `MSP-Podcast`
+ `evaluate.py` evaluates the CLAP model on emotion recognition on different datasets
  - Depending on which data has been used for training, this is may be a zero-shot scenario

Please find our best checkpoint at [HuggingFace](https://huggingface.co/KeiKinn/paraclap) trained and evalatuted on the MSP-Podcast. 